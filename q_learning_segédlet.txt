‚úÖ Q-learning seg√©dlet (ZH-hoz)
(Import enged√©lyezett, f√°jl .txt form√°tumban is haszn√°lhat√≥)

üìå F≈ëbb fogalmak:
- Q-learning: √©rt√©kalap√∫ meger≈ës√≠t√©ses tanul√°s, ahol egy Q[s,a] t√°bl√°t tanulunk.
- √Ållapot (state): a k√∂rnyezet adott helyzete.
- Akci√≥ (action): amit az √ºgyn√∂k tehet az adott √°llapotban.
- Jutalom (reward): az akci√≥ ut√°n kapott √©rt√©k (pl. +1, 0, -1).
- C√©l: megtanulni a legnagyobb kumulat√≠v jutalmat hoz√≥ akci√≥sorozatot.

üß† Q-t√°bla friss√≠t√©si k√©plete:
Q[s,a] ‚Üê Q[s,a] + Œ± * [r + Œ≥ * max Q[s',a'] - Q[s,a]]

- s: jelenlegi √°llapot  
- a: megtett akci√≥  
- r: kapott jutalom  
- s': √∫j √°llapot  
- Œ± (alpha): tanul√°si r√°ta (pl. 0.1)  
- Œ≥ (gamma): diszkont faktor (pl. 0.9)

‚öôÔ∏è Alap met√≥dusok:
def act(self, state):
    if random.random() < self.epsilon:
        return random.choice(self.actions)
    return np.argmax(self.Q[state])  # greedy akci√≥

def learn(self, state, action, reward, next_state):
    old_value = self.Q[state][action]
    future = max(self.Q[next_state])
    self.Q[state][action] = old_value + self.alpha * (reward + self.gamma * future - old_value)

üß± √Ållapott√©r reprezent√°ci√≥ (pl. Pong):
- √Ållapot: (labda_x, labda_y, sebess√©g_x, sebess√©g_y, √ºt≈ë_x)
- K√≥dol√°s: dictionary ‚Üí ID (pl. state_to_id[state])
- Q-t√°bla: Q[state_id][action]

üèÅ Termin√°lis √°llapot:
- Ha a labda leesik (vesz√≠t√ºnk) vagy a j√°t√©k v√©get √©r.

üîÅ P√©ld√°nyos√≠t√°s (epiz√≥d):
for ep in range(n_episodes):
    state = env.reset()
    while not done:
        action = agent.act(state)
        new_state, reward, done = env.step(action)
        agent.learn(state, action, reward, new_state)
        state = new_state

üéÆ P√©lda akci√≥k:
actions = [0, 1, 2]  # LEFT, RIGHT, IDLE

üì¶ Importok:
import random
import numpy as np
